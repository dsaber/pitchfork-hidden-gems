{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# standard\n",
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import nlp_processing as nlpp\n",
      "\n",
      "# storage\n",
      "import cPickle\n",
      "\n",
      "# Linear Regression + friends\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn import metrics\n",
      "import sklearn.cross_validation as cross_validation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf, logreg = cPickle.load(open('../tfidf_logreg.pkl', 'r'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv('../data/final_p4k.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "label = df['Score']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Here, I am essentially validating a tangential premise of my project -- \n",
      "# specifically, people are really good at explaining whether or not they like something\n",
      "# and why, but are terrible at assigning numeric values to that feeling (yes, even \n",
      "# people who are paid to assign numeric values to their feelings). \n",
      "\n",
      "# As we can see, when building a linear regression against review content, we see \n",
      "# that review content really only accounts for a small proportion of the overall score.\n",
      "# Contrast this with the validation we performed in 'search.py,' wherein we saw \n",
      "# that our logistic regression model was highly successful (.88 ROC-AUC/87% Accuracy)\n",
      "# in assigning \"Positive\"/\"Negative\" probabilities to reviews\n",
      "\n",
      "for train, test in cross_validation.KFold(df.shape[0], n_folds=3, shuffle=True):\n",
      "    print '*****New Fold*****'\n",
      "    train_content = df['Content'][train]\n",
      "    test_content  = df['Content'][test]\n",
      "    train_label   = label[train]\n",
      "    test_label    = label[test]\n",
      "    \n",
      "    tfidf = TfidfVectorizer(ngram_range=(1, 3), min_df=9)\n",
      "    transformed_train = tfidf.fit_transform(train_content)\n",
      "    transformed_test = tfidf.transform(test_content)\n",
      "    \n",
      "    clf = LinearRegression(fit_intercept=False)\n",
      "    clf.fit(transformed_train, train_label.ravel())\n",
      "    \n",
      "    print metrics.r2_score(test_label, clf.predict(transformed_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "*****New Fold*****\n",
        "0.197345269873"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*****New Fold*****\n",
        "0.226501344432"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*****New Fold*****\n",
        "0.194612793763"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    }
   ],
   "metadata": {}
  }
 ]
}