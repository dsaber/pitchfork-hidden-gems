{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# standard\n",
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import nlp_processing as nlpp\n",
      "\n",
      "# storage\n",
      "import cPickle\n",
      "\n",
      "# Linear Regression + friends\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "from sklearn import metrics\n",
      "import sklearn.cross_validation as cross_validation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf, logreg = cPickle.load(open('../tfidf_logreg.pkl', 'r'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv('../data/final_p4k.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "label = df['Score']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Here, I am essentially validating a tangential premise of my project -- \n",
      "# specifically, people are really good at explaining whether or not they like something\n",
      "# and why, but are terrible at assigning numeric values to that feeling (yes, even \n",
      "# people who are paid to assign numeric values to their feelings). \n",
      "\n",
      "# As we can see, when building a linear regression against review content, we see \n",
      "# that review content really only accounts for a relatively small proportion of the overall score.\n",
      "# Contrast this with the validation we performed in 'search.py,' wherein we saw \n",
      "# that our logistic regression model was highly successful (.88 ROC-AUC/87% Accuracy)\n",
      "# in assigning \"Positive\"/\"Negative\" probabilities to reviews\n",
      "\n",
      "for train, test in cross_validation.KFold(df.shape[0], n_folds=3, shuffle=True):\n",
      "    print '*****New Fold*****'\n",
      "    train_content = df['Content'][train]\n",
      "    test_content  = df['Content'][test]\n",
      "    train_label   = label[train]\n",
      "    test_label    = label[test]\n",
      "    \n",
      "    svd = TruncatedSVD(250)\n",
      "    tfidf = TfidfVectorizer(ngram_range=(1, 3), min_df=9)\n",
      "    transformed_train = svd.fit_transform(tfidf.fit_transform(train_content))\n",
      "    transformed_test = svd.transform(tfidf.transform(test_content))\n",
      "    \n",
      "    clf = RandomForestRegressor(50, n_jobs=-1)\n",
      "    clf.fit(transformed_train, train_label.ravel())\n",
      "    \n",
      "    print '\\nRandom Forest Regressor:'\n",
      "    print 'Train: ' + str(metrics.r2_score(train_label, clf.predict(transformed_train)))\n",
      "    print 'Test: ' +  str(metrics.r2_score(test_label, clf.predict(transformed_test)))\n",
      "    print 'Score: ' + str(clf.score(transformed_test, test_label))\n",
      "    \n",
      "    clf = LinearRegression()\n",
      "    clf.fit(transformed_train, train_label.ravel())\n",
      "    \n",
      "    print '\\nLinear Regression:'\n",
      "    print 'Train: ' + str(metrics.r2_score(train_label, clf.predict(transformed_train)))\n",
      "    print 'Test: ' +  str(metrics.r2_score(test_label, clf.predict(transformed_test)))\n",
      "    print 'Score: ' + str(clf.score(transformed_test, test_label)) + '\\n\\n'    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "*****New Fold*****\n",
        "\n",
        "Linear Regression:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train: 0.996111866297"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Test: 0.416976956442\n",
        "Score: 0.416976956442\n",
        "\n",
        "\n",
        "*****New Fold*****\n",
        "\n",
        "Linear Regression:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train: 0.999295388946\n",
        "Test: 0.397252874103"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Score: 0.397252874103\n",
        "\n",
        "\n",
        "*****New Fold*****\n",
        "\n",
        "Linear Regression:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train: 0.996931871999\n",
        "Test: 0.397208993987"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Score: 0.397208993987\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    }
   ],
   "metadata": {}
  }
 ]
}